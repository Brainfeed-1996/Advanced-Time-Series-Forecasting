{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Time Series Forecasting with LSTM\n",
    "\n",
    "**Author:** Olivier Robert-Duboille\n",
    "\n",
    "## 1. Introduction\n",
    "In this notebook, we explore the capabilities of Long Short-Term Memory (LSTM) networks for forecasting complex time series data. We will move beyond simple ARIMA models and dive into deep learning approaches that can capture non-linear temporal dependencies.\n",
    "\n",
    "### Objectives:\n",
    "- Generate a synthetic dataset representing realistic market trends with seasonality and noise.\n",
    "- Perform rigorous Exploratory Data Analysis (EDA) to understand stationarity and autocorrelation.\n",
    "- Build and train a stacked LSTM model using TensorFlow/Keras.\n",
    "- Evaluate performance using RMSE and visual inspection of forecast vs. actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation\n",
    "Real-world data is often messy. To ensure we have a clean ground truth for this demonstration, we'll generate a synthetic time series that combines:\n",
    "1. A linear trend\n",
    "2. A sinusoidal seasonal component\n",
    "3. Gaussian noise\n",
    "\n",
    "This simulates typical scenarios found in sales forecasting or stock trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # wave 2\n",
    "    series += 0.1 * (np.random.rand(n_steps) - 0.5)   # noise\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "\n",
    "# Generate data\n",
    "n_steps = 1000\n",
    "series = generate_time_series(n_steps)\n",
    "date_range = pd.date_range(start='2020-01-01', periods=n_steps, freq='D')\n",
    "df = pd.DataFrame(series, columns=['value'], index=date_range)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "Before modeling, we must visualize the data to understand its structure. We'll look at the raw series and the distribution of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df.index, df['value'], label='Observed Data', color='royalblue')\n",
    "plt.title('Synthetic Time Series Data', fontsize=16)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Distribution plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df['value'], kde=True, color='teal')\n",
    "plt.title('Distribution of Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "LSTMs are sensitive to the scale of input data. We'll normalize the data to the range [0, 1] using MinMaxScaler. We also need to reshape the data into a 3D format `[samples, time steps, features]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "def create_sequences(dataset, look_back=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        X.append(a)\n",
    "        y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 60\n",
    "X, y = create_sequences(scaled_data, look_back)\n",
    "\n",
    "# Reshape for LSTM [samples, time steps, features]\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Split into train and test\n",
    "train_size = int(len(X) * 0.8)\n",
    "test_size = len(X) - train_size\n",
    "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
    "y_train, y_test = y[0:train_size], y[train_size:len(y)]\n",
    "\n",
    "print(f\"Training Shape: {X_train.shape}, Testing Shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Building\n",
    "We use a Sequential model with two LSTM layers. The first returns sequences to feed into the second. Dropout layers are included to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(look_back, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=20, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "Visualizing the loss curve helps us check for convergence. Then, we predict on the test set and inverse transform the values to their original scale for interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "# Invert predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "y_train_inv = scaler.inverse_transform([y_train])\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "y_test_inv = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Calculate RMSE\n",
    "train_score = np.sqrt(mean_squared_error(y_train_inv[0], train_predict[:,0]))\n",
    "print(f'Train Score: {train_score:.2f} RMSE')\n",
    "test_score = np.sqrt(mean_squared_error(y_test_inv[0], test_predict[:,0]))\n",
    "print(f'Test Score: {test_score:.2f} RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "We have successfully built an LSTM model to forecast time series data. The plot below shows how our model's predictions align with the actual test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "train_plot = np.empty_like(scaled_data)\n",
    "train_plot[:, :] = np.nan\n",
    "train_plot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "\n",
    "test_plot = np.empty_like(scaled_data)\n",
    "test_plot[:, :] = np.nan\n",
    "test_plot[len(train_predict)+(look_back*2)+1:len(scaled_data)-1, :] = test_predict\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(scaler.inverse_transform(scaled_data), label='Actual Data')\n",
    "plt.plot(train_plot, label='Train Prediction')\n",
    "plt.plot(test_plot, label='Test Prediction')\n",
    "plt.title('LSTM Forecasting Results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}