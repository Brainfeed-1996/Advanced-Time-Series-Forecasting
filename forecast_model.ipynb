{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5eccf9",
   "metadata": {},
   "source": [
    "# Industrialized Time Series Forecasting: Bidirectional LSTM with Attention\n",
    "\n",
    "**Author:** Olivier Robert-Duboille\n",
    "**Date:** 2024-05-22\n",
    "**Version:** 2.0 (Industrialized)\n",
    "\n",
    "## 1. Abstract\n",
    "This notebook presents a production-grade workflow for time series forecasting. Moving beyond simple ARIMA or vanilla LSTM implementations, we construct a robust Deep Learning pipeline featuring:\n",
    "- **Bidirectional LSTMs** to capture temporal dependencies in both forward and backward directions.\n",
    "- **Custom Attention Mechanism** to weigh the importance of different time steps in the input sequence.\n",
    "- **Walk-Forward Validation (TimeSeriesSplit)** for realistic performance estimation.\n",
    "- **Hyperparameter Tuning** integration (stubbed for KerasTuner).\n",
    "- **Residual Analysis** to verify statistical assumptions.\n",
    "\n",
    "## 2. Methodology: The LSTM & Attention Mechanism\n",
    "\n",
    "### 2.1 Long Short-Term Memory (LSTM)\n",
    "The LSTM unit mitigates the vanishing gradient problem in RNNs using a gating mechanism. For a given time step $t$, the hidden state $h_t$ and cell state $c_t$ are updated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\quad &(\\text{Forget Gate}) \\\\\n",
    "i_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\quad &(\\text{Input Gate}) \\\\\n",
    "\\tilde{c}_t &= \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c) \\quad &(\\text{Candidate Cell}) \\\\\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\quad &(\\text{Cell State Update}) \\\\\n",
    "o_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\quad &(\\text{Output Gate}) \\\\\n",
    "h_t &= o_t \\odot \\tanh(c_t) \\quad &(\\text{Hidden State})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 2.2 Attention Mechanism\n",
    "To allow the model to focus on specific past time steps that are most relevant for the prediction, we apply an attention mechanism over the LSTM outputs $H = [h_1, h_2, ..., h_T]$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "e_t &= \\tanh(W_a h_t + b_a) \\\\\n",
    "\\alpha_t &= \\frac{\\exp(e_t)}{\\sum_{k=1}^{T} \\exp(e_k)} \\\\\n",
    "c &= \\sum_{t=1}^{T} \\alpha_t h_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where $c$ is the context vector passed to the final dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c091d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, Layer, Activation, Dot, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff91eb",
   "metadata": {},
   "source": [
    "## 3. Data Generation: Simulating Complex Realities\n",
    "We generate a synthetic time series that mimics complex financial or sensor data. It includes:\n",
    "1.  **Non-linear Trend**: A logistic growth trend.\n",
    "2.  **Multi-Seasonality**: Weekly and Monthly cycles.\n",
    "3.  **Heteroscedastic Noise**: Noise variance that changes over time.\n",
    "4.  **Structural Breaks**: Sudden shifts in the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc50e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complex_series(n_steps=2000):\n",
    "    t = np.arange(n_steps)\n",
    "    \n",
    "    # 1. Non-linear Trend (Logistic-like)\n",
    "    trend = 100 / (1 + np.exp(-(t - n_steps/2) / 400))\n",
    "    \n",
    "    # 2. Multi-Seasonality\n",
    "    seasonality_weekly = 10 * np.sin(2 * np.pi * t / 7)\n",
    "    seasonality_monthly = 20 * np.sin(2 * np.pi * t / 30)\n",
    "    \n",
    "    # 3. Heteroscedastic Noise (Variance increases with time)\n",
    "    noise_level = 2 + (t / n_steps) * 5\n",
    "    noise = np.random.normal(0, 1, n_steps) * noise_level\n",
    "    \n",
    "    # 4. Structural Break (Shift at t=1500)\n",
    "    break_point = 1500\n",
    "    structural_break = np.zeros(n_steps)\n",
    "    structural_break[break_point:] = 30\n",
    "    \n",
    "    series = trend + seasonality_weekly + seasonality_monthly + noise + structural_break\n",
    "    \n",
    "    date_range = pd.date_range(start='2018-01-01', periods=n_steps, freq='D')\n",
    "    return pd.DataFrame({'value': series}, index=date_range)\n",
    "\n",
    "df = generate_complex_series()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(df.index, df['value'], label='Complex Series', linewidth=1)\n",
    "plt.title('Synthetic Data with Trend, Seasonality, Heteroscedasticity, and Breaks', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c93c33",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering & Preprocessing\n",
    "Raw time series are rarely sufficient. We engineer features to help the LSTM converge faster:\n",
    "-   **Lag Features**: Values at $t-7, t-30$ (capturing seasonality).\n",
    "-   **Rolling Statistics**: Rolling mean and std dev to capture local trends/volatility.\n",
    "-   **Robust Scaling**: Using median and IQR to be robust against outliers (noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Lag features\n",
    "    data['lag_7'] = data['value'].shift(7)\n",
    "    data['lag_30'] = data['value'].shift(30)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    data['rolling_mean_7'] = data['value'].rolling(window=7).mean()\n",
    "    data['rolling_std_7'] = data['value'].rolling(window=7).std()\n",
    "    \n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "df_features = engineer_features(df)\n",
    "print(f\"Data shape after feature engineering: {df_features.shape}\")\n",
    "\n",
    "# Preprocessing\n",
    "scaler = RobustScaler()\n",
    "scaled_data = scaler.fit_transform(df_features)\n",
    "\n",
    "# Sequence Generation\n",
    "def create_sequences(data, seq_length, forecast_horizon=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - forecast_horizon + 1):\n",
    "        X.append(data[i:(i + seq_length)])\n",
    "        # Target: Predicting the 'value' column (index 0) 'forecast_horizon' steps ahead\n",
    "        y.append(data[i + seq_length + forecast_horizon - 1, 0]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LENGTH = 60\n",
    "X, y = create_sequences(scaled_data, SEQ_LENGTH)\n",
    "\n",
    "print(f\"Input Shape: {X.shape} (Samples, Timesteps, Features)\")\n",
    "print(f\"Target Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67756ede",
   "metadata": {},
   "source": [
    "## 5. Model Architecture: BiLSTM + Attention\n",
    "We implement a custom Attention layer inheriting from `tf.keras.layers.Layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', \n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer='normal')\n",
    "        self.b = self.add_weight(name='attention_bias', \n",
    "                                 shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros')\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape: (batch_size, time_steps, features)\n",
    "        # e = tanh(dot(x, W) + b)\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        \n",
    "        # alpha = softmax(e)\n",
    "        # Remove dimension of size 1\n",
    "        e = tf.squeeze(e, axis=-1) \n",
    "        alpha = tf.keras.backend.softmax(e)\n",
    "        \n",
    "        # context = sum(alpha * x)\n",
    "        alpha = tf.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = tf.keras.backend.sum(context, axis=1)\n",
    "        \n",
    "        return context\n",
    "\n",
    "def build_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Bidirectional LSTM to capture past and future context within the window\n",
    "    lstm_out = Bidirectional(LSTM(64, return_sequences=True))(inputs)\n",
    "    lstm_out = Dropout(0.3)(lstm_out)\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    lstm_out = Bidirectional(LSTM(32, return_sequences=True))(lstm_out)\n",
    "    lstm_out = Dropout(0.3)(lstm_out)\n",
    "    \n",
    "    # Attention Layer\n",
    "    attention_out = Attention()(lstm_out)\n",
    "    \n",
    "    # Dense Layers\n",
    "    x = Dense(32, activation='relu')(attention_out)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "model_example = build_model((SEQ_LENGTH, X.shape[2]))\n",
    "model_example.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee853c0",
   "metadata": {},
   "source": [
    "## 6. Walk-Forward Validation (Backtesting)\n",
    "Standard K-Fold is invalid for time series because it shuffles data. We use `TimeSeriesSplit` to respect temporal order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a84355",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "fold = 1\n",
    "rmse_scores = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = build_model((SEQ_LENGTH, X.shape[2]))\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0  # Silent training for cleanliness\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    preds = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Inverse Transform (trickier with RobustScaler on multiple features)\n",
    "    # We constructed X with multiple features, but y is just the first column (index 0)\n",
    "    # To inverse transform, we need a dummy array of the same shape as original input\n",
    "    \n",
    "    def inverse_transform_target(scaler, y_scaled, n_features):\n",
    "        dummy = np.zeros((len(y_scaled), n_features))\n",
    "        dummy[:, 0] = y_scaled.flatten()\n",
    "        return scaler.inverse_transform(dummy)[:, 0]\n",
    "\n",
    "    y_test_inv = inverse_transform_target(scaler, y_test, X.shape[2])\n",
    "    preds_inv = inverse_transform_target(scaler, preds, X.shape[2])\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test_inv, preds_inv))\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "    # Plot last fold\n",
    "    if fold == 3:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(y_test_inv, label='Actual', color='black', alpha=0.7)\n",
    "        plt.plot(preds_inv, label='Predicted', color='teal', alpha=0.7)\n",
    "        plt.title(f'Forecast vs Actual (Fold {fold})')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    fold += 1\n",
    "\n",
    "print(f\"\\nAverage RMSE: {np.mean(rmse_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25e2fa",
   "metadata": {},
   "source": [
    "## 7. Residual Analysis\n",
    "A good model should have residuals (errors) that resemble white noise (normally distributed, no autocorrelation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test_inv - preds_inv\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Distribution\n",
    "sns.histplot(residuals, kde=True, ax=ax[0], color='salmon')\n",
    "ax[0].set_title('Residual Distribution')\n",
    "ax[0].set_xlabel('Error')\n",
    "\n",
    "# Autocorrelation\n",
    "pd.plotting.autocorrelation_plot(residuals, ax=ax[1])\n",
    "ax[1].set_title('Residual Autocorrelation')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Residual: {np.mean(residuals):.4f}\")\n",
    "print(f\"Std Residual: {np.std(residuals):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
