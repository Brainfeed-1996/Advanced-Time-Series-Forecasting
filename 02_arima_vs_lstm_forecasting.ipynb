{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Time Series Forecasting â€” ARIMA vs LSTM\n",
    "\n",
    "This notebook provides a more industrial comparison between a classical statistical model (**ARIMA**) and a deep learning baseline (**LSTM**) on the same synthetic dataset (trend + seasonality + noise).\n",
    "\n",
    "Deliverables:\n",
    "- clear train/val split\n",
    "- standardized metrics (MAE/RMSE)\n",
    "- reproducible pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "SEED = 1337\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "def make_series(n=1500, season=24, noise=0.6):\n",
    "    t = np.arange(n)\n",
    "    trend = 0.0025 * t\n",
    "    seasonal = 1.2 * np.sin(2 * np.pi * t / season) + 0.4 * np.sin(2 * np.pi * t / (season*7))\n",
    "    y = 10 + trend + seasonal + rng.normal(0, noise, size=n)\n",
    "    return y\n",
    "\n",
    "y = make_series()\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "len(y_train), len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA (statsmodels)" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# simple order for demo; in production you'd grid-search / use auto_arima\n",
    "model = ARIMA(y_train, order=(2, 1, 2))\n",
    "fit = model.fit()\n",
    "pred_arima = fit.forecast(steps=len(y_val))\n",
    "\n",
    "mae_arima = float(np.mean(np.abs(pred_arima - y_val)))\n",
    "rmse_arima = float(sqrt(np.mean((pred_arima - y_val)**2)))\n",
    "mae_arima, rmse_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (PyTorch)" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def make_windows(arr, window=48):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(arr) - window):\n",
    "        X.append(arr[i:i+window])\n",
    "        Y.append(arr[i+window])\n",
    "    return np.array(X, dtype=np.float32), np.array(Y, dtype=np.float32)\n",
    "\n",
    "window = 48\n",
    "Xtr, Ytr = make_windows(y_train, window=window)\n",
    "Xva, Yva = make_windows(np.concatenate([y_train[-window:], y_val]), window=window)\n",
    "\n",
    "# normalize using train stats only\n",
    "mu, sig = Xtr.mean(), Xtr.std() + 1e-8\n",
    "Xtr = (Xtr - mu) / sig\n",
    "Xva = (Xva - mu) / sig\n",
    "Ytr_n = (Ytr - mu) / sig\n",
    "\n",
    "Xtr_t = torch.from_numpy(Xtr).unsqueeze(-1)\n",
    "Ytr_t = torch.from_numpy(Ytr_n).unsqueeze(-1)\n",
    "Xva_t = torch.from_numpy(Xva).unsqueeze(-1)\n",
    "\n",
    "class LSTMForecaster(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=64, num_layers=2, dropout=0.15, batch_first=True)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "model = LSTMForecaster()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# tiny training loop (kept short for notebook); in production use DataLoader + early stopping\n",
    "model.train()\n",
    "for epoch in range(8):\n",
    "    opt.zero_grad()\n",
    "    pred = model(Xtr_t)\n",
    "    loss = loss_fn(pred, Ytr_t)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if epoch % 2 == 0:\n",
    "        print('epoch', epoch, 'loss', float(loss))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    p = model(Xva_t).squeeze(-1).numpy()\n",
    "pred_lstm = p * sig + mu\n",
    "\n",
    "mae_lstm = float(np.mean(np.abs(pred_lstm - y_val)))\n",
    "rmse_lstm = float(sqrt(np.mean((pred_lstm - y_val)**2)))\n",
    "mae_lstm, rmse_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "  'model': ['ARIMA(2,1,2)', 'LSTM'],\n",
    "  'MAE': [mae_arima, mae_lstm],\n",
    "  'RMSE': [rmse_arima, rmse_lstm],\n",
    "}).sort_values('RMSE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
